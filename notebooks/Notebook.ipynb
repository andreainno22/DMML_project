{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "b3be9428d8ca11fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First clustering test, with kmeans and dbscan on the specific context: vary low variance, no relevant clusters are detected.\n",
    "‚úÖ Perch√© la varianza √® bassa nel tuo caso\n",
    "Stai analizzando comportamenti in contesti molto specifici (es. \"1st serve su terra\").\n",
    "\n",
    "In questi contesti, le strategie sono naturalmente simili tra giocatori: tutti affrontano condizioni e dinamiche simili.\n",
    "\n",
    "Quindi √® normale che le feature (rate di slice, vincenti, lunghezza media degli scambi, ecc.) abbiano variazioni minime, da cui una PCA schiacciata e clustering difficile.\n",
    "\n",
    "üß≠ Strategia: allargare progressivamente il contesto\n",
    "Ottima idea. Puoi procedere cos√¨:\n",
    "\n",
    "1. Contesto medio-specializzato\n",
    "Es. \"tutti i punti al servizio su terra\", unendo 1st e 2nd.\n",
    "\n",
    "2. Contesto pi√π ampio\n",
    "Es. \"tutti i punti al servizio\" indipendentemente dalla superficie.\n",
    "\n",
    "3. Contesto generale\n",
    "Tutti i colpi del giocatore, aggregando tutto (serve/response, superfici).\n",
    "\n",
    "üéØ Obiettivo finale:\n",
    "Capire a che livello di granularit√† emergono:\n",
    "\n",
    "differenze significative tra stili (pi√π cluster distinti, silhouette score alto)\n",
    "\n",
    "oppure confermare che alcuni contesti sono intrinsecamente ‚Äúuniformanti‚Äù (‚Üí bassa varianza, stile unico richiesto)."
   ],
   "id": "5486d49e16c8ec1b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Uploading of the all 6 dataset. Each dataset represents a context of type {point type}-{surface}.",
   "id": "97de80069a6830ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dfs = []\n",
    "features_dataset_dir = '../feature_datasets_csv_reduced_contexts&features/'\n",
    "\n",
    "for dataset_name in os.listdir(features_dataset_dir):\n",
    "    if dataset_name.endswith('.csv'):\n",
    "        file_path = os.path.join(features_dataset_dir, dataset_name)\n",
    "        dfs.append((dataset_name, pd.read_csv(file_path, low_memory=False)))\n",
    "umap_components = (2, 8)"
   ],
   "id": "377e8d02c2d906b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Variance visualization and hopkins statistic. A 0.75 Hopkins stat indicates clustering tendency with a 90% confidence level.",
   "id": "a5bb92aea5ecf847"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from run_clustering import hopkins\n",
    "\n",
    "for name, dataframe in dfs:\n",
    "    feature_cols = [col for col in dataframe.columns if col not in [\"player\"]]\n",
    "    print(name)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(dataframe[feature_cols].values)\n",
    "    dataframe[feature_cols].var().sort_values()\n",
    "    mean, std_dev, values = hopkins(X_scaled)\n",
    "    print(f\"Hopkins statistic (df): {mean:.3f} ¬± {std_dev:.3f}\\n\")"
   ],
   "id": "3bc9f8b21d5d9ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "UMAP.",
   "id": "eeaf485c9a2c1878"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from run_clustering import continuity\n",
    "from sklearn.manifold import trustworthiness\n",
    "import umap\n",
    "\n",
    "for name, dataframe in dfs:\n",
    "    feature_cols = [col for col in dataframe.columns if col not in [\"player\"]]\n",
    "    # z-score normalization\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(dataframe[feature_cols])\n",
    "    for umap_comp in umap_components:\n",
    "        # UMAP reduction\n",
    "        reducer = umap.UMAP(n_components=umap_comp)\n",
    "        X_umap = reducer.fit_transform(X_scaled)\n",
    "        c_score = continuity(X_scaled, X_umap, n_neighbors=15)\n",
    "        print(f\"Continuity: {c_score:.3f}\")\n",
    "        trust = trustworthiness(X_scaled, X_umap, n_neighbors=15)\n",
    "        print(f\"Trustworthiness UMAP {umap_comp}D: {trust:.3f}\")"
   ],
   "id": "58c6c4582595d35a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "KMeans clustering:",
   "id": "a83ebe7a0ef234ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from run_clustering import *\n",
    "\n",
    "dfs_clustered = []\n",
    "\n",
    "for name, dataframe in dfs:\n",
    "    print(name)\n",
    "    for umap_comp in umap_components:\n",
    "        metrics = []\n",
    "        df_clustered, silhouette_score, dbi = run_kmeans_clustering(dataframe, 4, True, umap_comp)\n",
    "        dfs_clustered.append((name, df_clustered))\n",
    "        metrics.append((silhouette_score, dbi, umap_comp))\n",
    "    print({name: max(metrics, key=lambda x: x[0])})\n"
   ],
   "id": "2f9452a54752820e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Agglomerative clustering:",
   "id": "b2cbc4b056ea3c4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from run_clustering import *\n",
    "\n",
    "for name, dataframe in dfs:\n",
    "    print(name)\n",
    "    for umap_comp in umap_components:\n",
    "        silhouette_scores = []\n",
    "        df_clustered, silhouette_score, dbi = run_umap_agglomerative_clustering(dataframe, \"player\", 4, 2, 'complete',\n",
    "                                                                                True)\n",
    "        silhouette_scores.append((silhouette_score, umap_comp))\n",
    "    print({name: max(silhouette_scores, key=lambda x: x[1])})"
   ],
   "id": "de596699b8daf97d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Optics reachability plot:",
   "id": "83601a6eaa50b37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from run_clustering import *\n",
    "\n",
    "for name, dataframe in dfs:\n",
    "    print(name)\n",
    "    for min_pts in range(8, 9):\n",
    "        for xi in np.arange(start=0.1, stop=0.11, step=0.01):\n",
    "            df_clustering = run_umap_optics_clustering(dataframe, min_pts=min_pts, xi=xi, context_cols=\"player\")"
   ],
   "id": "247fd93e184344a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "GMM cluster:",
   "id": "393c88d3dcc9471a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from run_clustering import run_umap_gmm_clustering\n",
    "\n",
    "for umap_comp in umap_components:\n",
    "    for name, dataframe in dfs:\n",
    "        print(name)\n",
    "        for umap_knn in [10, 15, 20, 25, 30]:\n",
    "            print(\"nearest neighbors umap: \", umap_knn)\n",
    "            run_umap_gmm_clustering(dataframe, context_cols=\"player\", umap_dim=umap_comp, visualize=False)\n"
   ],
   "id": "4e3d1abc206a2ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Agglomerate statistic by cluster visualization:",
   "id": "df4906fbf74618f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from visualize_results import cluster_feature_deltas\n",
    "from visualize_results import aggregate_features_by_cluster\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# stampa le features aggregate per cluster\n",
    "aggregated = aggregate_features_by_cluster(df_clustered)\n",
    "print(aggregated)\n",
    "\n",
    "# stampa la hitmap della differenza nelle statistiche tra i clusters e la media globale\n",
    "sns.heatmap(cluster_feature_deltas(df_clustered), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Scostamenti medi delle feature per cluster\")\n",
    "plt.show()\n"
   ],
   "id": "79ea9c52117db76b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Save dfs clustered in files.",
   "id": "3f6f19810eea7d6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "features_dataset_clustered_dir = '../feature_datasets_csv_reduced_contexts&features_clustered/'\n",
    "for name, df_clustered in dfs_clustered:\n",
    "    file_path = os.path.join(features_dataset_clustered_dir, name)\n",
    "    df_clustered.to_csv(file_path, index=False)"
   ],
   "id": "4c0e26b43fa200fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Upload dfs clustered.",
   "id": "acd1da5b35010d56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "features_dataset_clustered_dir = '../feature_datasets_csv_reduced_contexts&features_clustered/'\n",
    "dfs_clustered = []\n",
    "for dataset_name in os.listdir(features_dataset_clustered_dir):\n",
    "    if dataset_name.endswith('.csv'):\n",
    "        file_path = os.path.join(features_dataset_clustered_dir, dataset_name)\n",
    "        dfs_clustered.append((dataset_name, pd.read_csv(file_path)))"
   ],
   "id": "9617d46a04b7bd3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate for each context, for each cluster, the top n representative features, i.e. the ones that have the highest difference between the cluster centroid feature and the overall feature mean.",
   "id": "d77d284f3a571c30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from visualize_results import analyze_cluster_profiles\n",
    "\n",
    "for name, df_clustered in dfs_clustered:\n",
    "    print(name)\n",
    "    analyze_cluster_profiles(df_clustered, context=name.replace(\".csv\", \"\"))"
   ],
   "id": "ceb75843b14ea735",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Visualize similarity matrix and heatmap. The similarity matrix calculate the distance between each cluster in a context with each other cluster in different context. For each cluster, the 3 most similar clusters in each the 3 surfaces represent the same style",
   "id": "6eed2ad5b42959de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from visualize_results import calculate_centroid_similarity\n",
    "\n",
    "# 3. Calcolo della Similarit√† tra Centroidi\n",
    "clustered_data_on_serve = []\n",
    "clustered_data_on_response = []\n",
    "for context, df_clustered in dfs_clustered:\n",
    "    if \"on serve\" in context:\n",
    "        clustered_data_on_serve.append((context, df_clustered))\n",
    "    else:\n",
    "        clustered_data_on_response.append((context, df_clustered))\n",
    "similarity_df_on_serve = calculate_centroid_similarity(clustered_data_on_serve)\n",
    "similarity_df_on_response = calculate_centroid_similarity(clustered_data_on_response)"
   ],
   "id": "8d7adf8e9f28f23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from visualize_results import visualize_similarity_matrix\n",
    "\n",
    "# 4. Visualizzazione della Matrice di Similarit√†\n",
    "visualize_similarity_matrix(similarity_df_on_serve, clustered_data_on_serve)\n",
    "visualize_similarity_matrix(similarity_df_on_response, clustered_data_on_response)"
   ],
   "id": "1f26332ca521140a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Calculate for each point context (serve, response) 4 triplets where 4 is the number of playing styles. Each triplet is composed by a cluster for each surface. The goal is to find the best similarity of each cluster of a surface with the others clusters of the other surfaces.",
   "id": "5d0fa86e9c75e263"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#todo probabilmente non serve a niente, modificare in modo tale da stampare per ogni contest/cluster i due cluster sulle diverse superfici piu simili\n",
    "from visualize_results import find_closest_cluster_triplets\n",
    "\n",
    "on_serve_contexts = [context for context, _ in dfs_clustered if \"on serve\" in context]\n",
    "on_response_contexts = [context for context, _ in dfs_clustered if \"on response\" in context]\n",
    "print(find_closest_cluster_triplets(similarity_df_on_serve, on_serve_contexts))\n",
    "print(find_closest_cluster_triplets(similarity_df_on_response, on_response_contexts))\n"
   ],
   "id": "687b1ed67a9b6019",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Visualize top players trajectories.",
   "id": "47a1d694beff13e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from visualize_results import create_player_trajectories, display_top_player_trajectories\n",
    "\n",
    "display_top_player_trajectories(create_player_trajectories(dfs_clustered))"
   ],
   "id": "cc6563a25f264695",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Visualize player trajectory by name.",
   "id": "4759717984790027"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from visualize_results import create_player_trajectories, visualize_player_trajectory\n",
    "\n",
    "visualize_player_trajectory(create_player_trajectories(dfs_clustered), \"Roger_Federer\", dfs_clustered,\n",
    "                            top_features=True)"
   ],
   "id": "8ad47f1675b03c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Visualize most frequent patterns.",
   "id": "5aa4cfcd62307fb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from costants import PLAYER_SURFACES_DICT\n",
    "from decode_point import decode_point\n",
    "from get_freq_shots_seqs import frequent_shots_py_player\n",
    "\n",
    "df_points = pd.read_csv('../points_datasets/charting-m-points-2020s.csv', low_memory=False)\n",
    "\n",
    "print(\"Legend: * = winner, # = forced error, @ = unforced error\\n\")\n",
    "for surface in PLAYER_SURFACES_DICT.get(\"Carlos_Alcaraz\", []):\n",
    "    for context in [\"on serve\", \"on response\"]:\n",
    "        print(\"\\n\"+context + \" on \" + surface + \"\\n\")\n",
    "        for sequence, support, win_percentage, most_frequent_outcome in frequent_shots_py_player(\"Carlos_Alcaraz\",df_points, surface, context):\n",
    "            shots = []\n",
    "            for shot in sequence:\n",
    "                shots.append(decode_point(shot))\n",
    "            print(\n",
    "                f\"Sequence: {shots} | Support: {support} | Win percentage: {win_percentage} | Most frequent outcome: {most_frequent_outcome}\\n\")\n"
   ],
   "id": "8fd1c4250a1f5e9c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

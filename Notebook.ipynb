{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3be9428d8ca11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486d49e16c8ec1b",
   "metadata": {},
   "source": [
    "<span style=\"font-size:35px\">**Contextual Analysis of Tennis Playing Styles**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817062d58a66e5bb",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px\">**Introduction**</span>\n",
    "\n",
    "This notebook presents a context-specific analysis of tennis playing styles, using **clustering** and a **sequential pattern mining** analysis. The main goal is to identify and analyze tactical playing styles in professional tennis using real shot-level data, with a focus on how these styles vary by match context.\n",
    "\n",
    "Regarding the **clustering analysis** the key idea is, instead of building a single playing profile per player, analyzing each player's behavior separately in distinct tactical contexts, defined by:\n",
    "* **Surface**: clay, hard, grass\n",
    "* **Point situation**: serve/return\n",
    "\n",
    "For a total of 6 independent contexts, each with its own clustering.\n",
    "\n",
    "This type of analysis can have a role for:\n",
    "* **Match preparation**: have an overview of the opponent playing style.\n",
    "* **Evolution tracking**: track how a playerâ€™s tactical style evolves over time or after coaching changes.\n",
    "\n",
    "\n",
    "The goal of the **sequential pattern analysis** instead, is to identify frequent shot sequences in the opening phase of a point for a given player, along with information about their support, win percentage, and most frequent outcome. This helps the player understand how they perform in different contexts, and which strategies are effective or ineffective.\n",
    "\n",
    "\n",
    "First, a preprocessing phase is performed on the point dataset obtained from https://github.com/JeffSackmann/tennis_MatchChartingProject. This dataset includes a large number of points recorded from 2020 to the end of 2024 from the ATP World Tour, ATP Challenger Tour and ITF tour, the top level tennis tournament categories, with each record containing the complete sequence of shots for a point, along with metadata such as tournament name, player names, and more. The list of points is ordered so that the points belonging to a specific match appear in the sequence they were played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c78f281dd48404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_men = pd.read_csv('points_datasets/charting-m-points-2020s-original.csv', low_memory=False)\n",
    "\n",
    "df_men"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec36f94",
   "metadata": {},
   "source": [
    "In tennis, during each point, a player can be either the server or the returner. The server has two chances to land a valid serve, so the point can start with either a first or a second serve.\n",
    "The fields **1st** and **2nd** represent the sequence of shots played following a successful first or second serve, respectively. If the first serve is in, the 2nd field will be NaN. If the first serve is out, the 1st field will still be recorded to indicate the attempted serve.<br>\n",
    "The entire sequence of shots is codified with alphanumeric characters, each one represents a specific feature of the shot:\n",
    "* **type**: forehand, backand, slice, dropshot, ...\n",
    "* **direction**: down the line, in the middle, crosscourt\n",
    "* **serve direction**: in the middle, out wide, ...\n",
    "* **serve outcome**: ace, fault type\n",
    "* **serve response depth**: deep, mid, short\n",
    "* **point outcome**: winner, forced error, unforced error\n",
    "* other particular events.\n",
    "\n",
    "The file [get_mapping_dictionaries.py](get_mapping_dictionaries.py) contains a complete codification mapping of alphanumeric characters.<br><br>\n",
    "\n",
    "<span style=\"font-size:20px\">**Clustering Analysis**</span>\n",
    "\n",
    "<span style=\"font-size:15px\">**Surface field building**</span>\n",
    "\n",
    "The original dataset does not contain information about the surface, so that a manual feature creation is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab187a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_men = pd.read_csv('../points_datasets/charting-m-points-2020s-original.csv', low_memory=False)\n",
    "tournaments = df_men[\"match_id\"].str.extract(r'\\w*-\\w*-(.*?)-.*')[0].unique()\n",
    "\n",
    "\n",
    "tournaments_and_surfaces={'United_Cup':\"hard\", 'NextGen_Finals':\"hard\", 'Yokkaichi_CH':\"hard\", 'Temuco_CH':\"hard\",\n",
    "       'Maia_CH':\"clay\", 'Puerto_Vallarta_CH':\"hard\", 'Davis_Cup_Finals':\"hard\", 'Montemar_CH':\"clay\",\n",
    "       'Roverto_CH':\"hard\", 'Yokohama_CH':\"hard\", 'Tour_Finals':\"hard\", 'Champaign_CH':\"hard\",\n",
    "       'Helsinki_CH':\"hard\", 'Knoxville_CH':\"hard\", 'Paris_Masters':\"hard\", 'Brazzaville_CH':\"clay\",\n",
    "       'Guayaquil_CH':\"clay\", 'Bratislava_2_CH':\"clay\", 'Charlottesville_CH':\"hard\",\n",
    "       'Sydney_CH':\"hard\", 'Basel':\"hard\", 'Curitiba_CH':\"clay\", 'Brest_CH':\"hard\", 'Vienna':\"hard\",\n",
    "       'Six_Kings_Slam':\"hard\", 'Antwerp':\"hard\", 'Stockholm':\"hard\", 'Campinas_CH':\"clay\",\n",
    "       'Villa_Maria_CH':\"clay\", 'Shanghai_Masters':\"hard\", 'Tiburon_CH':\"hard\", 'Beijing':\"hard\",\n",
    "       'Tokyo':\"hard\", 'Antofagasta_CH':\"clay\", 'Hangzhou':\"hard\", 'Saint_Tropez_CH':\"hard\",\n",
    "       'Chengdu':\"hard\", 'Davis_Cup_World_Group':\"hard\", 'Dobrich_CH':\"clay\", 'US_Open':\"hard\",\n",
    "       'Winston_Salem':\"hard\", 'Cincinnati_Masters':\"hard\", 'Canada_Masters':\"hard\",\n",
    "       'Bogota_CH':\"clay\", 'Lincoln_CH':\"hard\", 'Lexington_CH':\"hard\", 'Olympics':\"hard\",\n",
    "       'Luedenscheid':\"clay\", 'Washington':\"hard\", 'Paris_Olympics':\"clay\", 'Umag':\"clay\",\n",
    "       'Chicago_CH':\"hard\", 'Atlanta':\"hard\", 'Hamburg':\"grass\", 'Gstaad':\"clay\", 'Newport':\"grass\",\n",
    "       'Granby_CH':\"hard\", 'Bastad':\"clay\", 'Braunschweig_CH':\"clay\", 'Wimbledon':\"grass\",\n",
    "       'Winnipeg_CH':\"hard\", 'ITF_Tokyo':\"hard\", 'Karlsruhe_CH':\"clay\", 'Bloomfield_Hills_CH':\"hard\",\n",
    "       'Eastbourne':\"grass\", 'Queens_Club':\"grass\", 'Halle':\"grass\", 'Sassuolo_CH':\"clay\",\n",
    "       's_Hertogenbosch':\"grass\", 'Stuttgart':\"grass\", 'Roland_Garros':\"clay\",\n",
    "       'Roland_Garros_Juniors':\"clay\", 'Prostejov_CH':\"clay\", 'Vicenza_CH':\"clay\", 'Geneva':\"clay\",\n",
    "       'Rome_Masters':\"clay\", 'Bordeaux_CH':\"clay\", 'Madrid_Masters':\"clay\", 'Cagliari_CH':\"clay\",\n",
    "       'Guangzhou_CH':\"hard\", 'Madrid':\"clay\", 'Munich':\"clay\", 'Bucharest':\"clay\", 'Barcelona':\"clay\",\n",
    "       'Acapulco_CH':\"hard\", 'Monte_Carlo_Masters':\"clay\", 'Estoril':\"clay\", 'Barcelona_CH':\"clay\",\n",
    "       'Miami_Masters':\"hard\", 'San_Luis_Potosi_CH':\"clay\", 'Indian_Wells_Masters':\"hard\",\n",
    "       'Sao_Leopoldo_CH':\"hard\", 'Asuncion_CH':\"clay\", 'Szekesfehervar_CH':\"clay\", 'Santiago':\"clay\",\n",
    "       'Dubai':\"hard\", 'Acapulco':\"hard\", 'Rio_de_Janeiro':\"clay\", 'Los_Cabos':\"hard\", 'Doha':\"hard\",\n",
    "       'Rotterdam':\"hard\", 'Delray_Beach':\"hard\", 'Buenos_Aires':\"clay\", 'Marseille':\"hard\",\n",
    "       'Cordoba':\"clay\", 'Dallas':\"hard\", 'Montpellier':\"hard\", 'Davis_Cup_Qualifiers':\"hard\",\n",
    "       'Cleveland_CH':\"hard\", 'Australian_Open':\"hard\", 'Australian_Open_Juniors':\"hard\",\n",
    "       'Quimper_CH':\"hard\", 'Buenos_Aires_CH':\"clay\", 'Adelaide':\"hard\", 'Auckland':\"hard\",\n",
    "       'Brisbane':\"hard\", 'Hong_Kong':\"hard\", 'Dutch_Championships':\"clay\", 'Ismaning_CH':\"grass\",\n",
    "       'Astana':\"hard\", 'Kitzbuhel':\"clay\", 'Amersfoort_CH':\"clay\", 'Lyon':\"clay\", 'Rome':\"clay\",\n",
    "       'Aix_En_Provence_CH':\"clay\", 'Rome_CH':\"clay\", 'Banja_Luka':\"clay\", 'Marrakech':\"clay\",\n",
    "       'Houston':\"clay\", 'Manama_CH':\"hard\", 'Davis_Cup_WG2':\"hard\", 'Tigre_CH':\"clay\", 'Oeiras_CH':\"clay\",\n",
    "       'Nonthaburi_CH':\"hard\", 'Pune':\"hard\", 'Valencia_CH':\"clay\", 'Roanne_CH':\"hard\", 'Bergamo_CH':\"hard\",\n",
    "       'Vilnius_CH':\"hard\", 'Naples':\"clay\", 'Naples_CH':\"clay\", 'Ortisei_CH':\"hard\", 'Gijon':\"hard\",\n",
    "       'Florence':\"hard\", 'Mouilleron_CH':\"hard\", 'Alicante_CH':\"clay\", 'Tel_Aviv':\"hard\", 'Sofia':\"hard\",\n",
    "       'Seoul':\"hard\", 'Orleans_CH':\"hard\", 'Metz':\"hard\", 'Laver_Cup':\"hard\", 'San_Diego':\"hard\",\n",
    "       'Rennes_CH':\"hard\", 'Tulln_CH':\"clay\", 'San_Benedetto_del_Tronto_CH':\"clay\",\n",
    "       'Indianapolis_CH':\"hard\", 'Verona_CH':\"clay\", 'Rome_GA_CH':\"clay\", 'Wimbledon_Juniors':\"grass\",\n",
    "       'Corrientes_CH':\"clay\", 'Shymkent_CH':\"clay\", 'Zagreb_CH':\"clay\", 'Belgrade':\"hard\",\n",
    "       'Barletta_CH':\"clay\", 'Lugano_CH':\"hard\", 'Zadar_CH':\"clay\", 'ITF_Santo_Domingo':\"clay\",\n",
    "       'Forli_CH':\"clay\", 'Sydney':\"hard\", 'Melbourne':\"hard\", 'ATP_Cup':\"hard\", 'French_Club':\"clay\",\n",
    "       'Pau_CH':\"hard\", 'Bratislava_CH':\"clay\", 'Tenerife_CH':\"hard\", 'Eckental_CH':\"grass\",\n",
    "       'St_Petersburg':\"hard\", 'Ercolano_CH':\"clay\", 'Villena_CH':\"hard\", 'Napoli_CH':\"clay\",\n",
    "       'Santiago_CH':\"clay\", 'Murcia_CH':\"clay\", 'Davis_Cup_Group_I':\"hard\", 'Segovia_CH':\"hard\",\n",
    "       'ITF_Champaign':\"hard\", 'Tokyo_Olympics':\"hard\", 'Poznan_CH':\"clay\", 'ITF_The_Hague':\"clay\",\n",
    "       'ITF_Alkmaar':\"clay\", 'Almaty_CH':\"hard\", 'Lyon_CH':\"clay\", 'Little_Rock_CH':\"hard\",\n",
    "       'Prague_CH':\"clay\", 'Orlando_CH':\"hard\", 'Split_CH':\"clay\", 'Lille_CH':\"hard\", 'Singapore':\"hard\",\n",
    "       'Great_Ocean_Road_Open':\"hard\", 'Antalya':\"clay\", 'Lima_CH':\"clay\", 'Cologne':\"hard\",\n",
    "       'Davis_Cup_WG_II':\"hard\", 'Columbus_CH':\"hard\", 'Koblenz_CH':\"hard\", 'New_York':\"hard\",\n",
    "       'Punta_del_Este_CH':\"clay\", 'Bangkok_CH':\"hard\", 'Noumea_CH':\"hard\"}\n",
    "\n",
    "# Directly map surface type using regular expression and dictionary\n",
    "df_men['surface'] = df_men['match_id'].str.extract(r'\\w*-\\w*-(.*?)-.*')[0].map(tournaments_and_surfaces)\n",
    "\n",
    "# Save the updated DataFrame with the 'surface' column to a new file\n",
    "df_men.to_csv('charting-m-points-2020s.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097733ff",
   "metadata": {},
   "source": [
    "<span style=\"font-size:15px\">**Extracting point sequence from the original dataset**</span>\n",
    "\n",
    "Both clustering and sequential pattern analysis are performed using the information provided by the shot codifications. In this preprocessing phase the usefeful information from the **1st** and **2nd** field of the original dataset are extracted, then the datasets used for clustering are created.<br>\n",
    "\n",
    "First of all, we iterate throw all the players and the surfaces. Using the functions `get_service_points_df()` and `get_response_points_df()` in the file [get_df_by_player.py](get_df_by_player.py), the dataset filtered by the current player and surface is calculated. Then using `get_shots_in_1st_serve_points()` and `get_shots_in_2nd_serve_points()` in the file [get_shots.py](get_shots.py), a new dataframe is built, containing three fields:\n",
    "\n",
    "* `point`: a list of codes each one represents a shot in the point\n",
    "* `won_by_player`: True if won, False otherwise\n",
    "* `outcome`: outcome type (winner, forced error, unforced error).\n",
    "\n",
    "Since the dataset still includes both the player's and the opponent's shots, a new transformation is applied using dedicated functions `get_shots_by_server` and `get_shots_by_receiver`, from the file [get_shots_wo_opponent_shots.py](get_shots_wo_opponent_shots.py), in order to obtain only the player's shots.\n",
    "\n",
    "A dictionary `player_contexts_data` contains the player shots for the specific context, where the context is the couple *surface={clay, hard, grass}* and *type of point={on serve, on response}*.\n",
    "\n",
    "<span style=\"font-size:15px\">**Feature building**</span>\n",
    "\n",
    "Now a new dataset for clustering purpose can be built. In particular 2 type of features are built: \n",
    "* **Generic features**: (`build_generic_features()`, file [feature_building.py](feature_building.py))\n",
    "    * `average_point_length`: reflects the player tendency either to conclude fast the point, or to be more conservative.\n",
    "    * `net_points_rate`: an high rate reflects a familiarity with the net approach, that usually corresponds to an offensive playing style.\n",
    "    * `net_points_won_rate`: an high rate reflects good skills with volleys.\n",
    "    * `winners_rate`: an high rate corresponds to an offensive playing style.\n",
    "    * `unforced_errors_rate`: an high rate corresponds to an offensive playing style.\n",
    "    * `slices_rate`: an high rate reflects variety in player style. \n",
    "    * `dropshots_rate`: an high rate reflects a eclectic style.\n",
    "    * `crosscourt_rate`: useful info.\n",
    "    * `middlecourt_rate`: useful info.\n",
    "    * `down_the_line_rate`: useful info.\n",
    "    * `ace_rate`: computed only for \"on serve\" context. An high rate corresponds to a \"big server\" style.\n",
    "    * `average_response_depth`: computed only for \"on response\" context. The deeper the response is, the better is the player on response.\n",
    "<br>\n",
    "<br>\n",
    "    \n",
    "    \n",
    "* **Opening phase features**: (`build_opening_phase_features()`, file [feature_building.py](feature_building.py)) This group of features captures the playerâ€™s behavior during the first three shots of a point. This is an important indicator of playing style, as the opening phase has the greatest influence on how the point unfolds. Each of these features is computed for all three shots, resulting in a total of nine features.\n",
    "    * `forehand_ground_rate`\n",
    "    * `backhand_ground_rate`\n",
    "    * `slice_shot_rate`\n",
    "    \n",
    "    Serve direction features are constructed only for points where the serve is the first shot.\n",
    "    * `wide_serve_rate`\n",
    "    * `body_serve_rate`\n",
    "    * `down_the_T_serve_rate`\n",
    "    \n",
    "<span style=\"font-size:15px\">**Final datasets building**</span>\n",
    "\n",
    "Once the features for each context are extracted, the final datasets are created. There will be six final datasets for clustering, corresponding to all possible combinations of *surface={clay, hard, grass}* and *type of point={on serve, on response}*, where each tuple identifies a player. (`build_final_dataset`, in file [feature_building.py](feature_building.py)).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1cf521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from costants import PLAYERS, MIN_PLAYERS, PLAYER_SURFACES_DICT, MIN_NUM_OF_POINTS\n",
    "from feature_building import build_final_dataset, build_generic_features, \\\n",
    "    build_opening_phase_features\n",
    "from utils import *\n",
    "from get_df_by_player import get_service_points_df, get_response_points_df\n",
    "from get_shots import get_shots_in_2nd_serve_points, get_shots_in_1st_serve_points\n",
    "from get_shots_wo_opponent_shots import get_shots_by_receiver, get_shots_by_server\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "all_features_by_context = defaultdict(list)  # Dizionario per accumulare features per contesto base\n",
    "\n",
    "for player in PLAYERS:\n",
    "    player_contexts_data = {}  # Dizionario temporaneo per dati del giocatore corrente\n",
    "\n",
    "    for surface in PLAYER_SURFACES_DICT.get(player, []):\n",
    "        # estraggo il dataframe con i punti in cui player Ã¨ al servizio\n",
    "        player_on_service = get_service_points_df(player, df_men, surface)\n",
    "\n",
    "        # estraggo il dataframe con i punti in cui player Ã¨ in risposta\n",
    "        player_on_response = get_response_points_df(player, df_men, surface)\n",
    "\n",
    "        # estraggo i punti in cui il giocatore Ã¨ al servizio\n",
    "        shots_1st_service = get_shots_by_server(get_shots_in_1st_serve_points(player_on_service))\n",
    "        shots_2nd_service = get_shots_by_server(get_shots_in_2nd_serve_points(player_on_service))\n",
    "\n",
    "        # estraggo i punti in cui il giocatore Ã¨ in risposta\n",
    "        shots_1st_response = get_shots_by_receiver(get_shots_in_1st_serve_points(player_on_response))\n",
    "        shots_2nd_response = get_shots_by_receiver(get_shots_in_2nd_serve_points(player_on_response))\n",
    "\n",
    "        player_contexts_data.update({f\"{player} on serve, on {surface}\": (pd.concat([shots_1st_service.copy(),\n",
    "                                                                                     shots_2nd_service.copy()]),\n",
    "                                                                          pd.concat([shots_1st_service.copy(),\n",
    "                                                                                     shots_2nd_service.copy()])),\n",
    "                                     f\"{player} on response, on {surface}\": (\n",
    "                                         pd.concat([shots_1st_response.copy(), shots_2nd_response.copy()]),\n",
    "                                         pd.concat([shots_1st_response.copy(), shots_2nd_response.copy()]))})\n",
    "        \n",
    "        # itera per ogni context, ovvero ogni punto di vista (serve o response) e per ogni colpo (1st o 2nd)\n",
    "        for context, (shots, shots_to_filter) in player_contexts_data.items():\n",
    "            if len(shots) < MIN_NUM_OF_POINTS:\n",
    "                continue\n",
    "\n",
    "            # filtro i colpi per >= di SHOT_LENGTH e taglio ai primi SHOT_LENGTH\n",
    "            filtered_shots = filter_and_trim_shots(shots_to_filter)\n",
    "            if len(filtered_shots) < MIN_NUM_OF_POINTS:\n",
    "                continue\n",
    "\n",
    "            # costruisce le features generiche usando tutti i punti\n",
    "            generic_features = build_generic_features(player, context, shots)\n",
    "\n",
    "            # costruisce le features di apertura usando i punti filtrati\n",
    "            opening_phase_features = build_opening_phase_features(context, filtered_shots)\n",
    "\n",
    "            # Controlla eventuali chiavi duplicate\n",
    "            common_keys = set(generic_features.keys()) & set(opening_phase_features.keys())\n",
    "            if common_keys:\n",
    "                print(\n",
    "                    f\"Attenzione: Chiavi duplicate trovate tra generic e opening features per {context}: {common_keys}\")\n",
    "\n",
    "            # Combina le features in un unico dizionario\n",
    "            combined_features = {**generic_features, **opening_phase_features, 'player': player}\n",
    "\n",
    "            # Estrai il contesto base (senza il nome del giocatore) per usarlo come chiave\n",
    "            # Raggruppa i dati di giocatori diversi per lo stesso tipo di contesto\n",
    "            try:\n",
    "                # Trova la prima occorrenza del nome del giocatore seguito da spazio e prendi il resto\n",
    "                base_context = context.split(f\"{player} \", 1)[1]\n",
    "                print(f\"Contesto per {player}: {base_context}\")\n",
    "            except IndexError:\n",
    "                # Fallback nel caso improbabile che il formato del contesto sia diverso\n",
    "                print(f\"Attenzione: formato contesto non standard '{context}'\")\n",
    "                base_context = context\n",
    "\n",
    "                # Aggiungi le features combinate alla lista per il contesto base corrispondente\n",
    "            all_features_by_context[base_context].append(combined_features.copy())\n",
    "            \n",
    "# Build all the final datasets \n",
    "final_datasets = build_final_dataset(all_features_by_context)\n",
    "\n",
    "# Crea una directory per salvare i file CSV, se non esiste\n",
    "output_dir = \"feature_datasets_csv_reduced_contexts\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Itera su ogni elemento del dizionario e salva i DataFrame in file CSV\n",
    "for name, df in final_datasets.items():\n",
    "    # Sostituisci eventuali caratteri non validi nel nome del file\n",
    "    safe_name = re.sub(r'[^\\w\\-_\\. ]', '_', name)\n",
    "    file_path = os.path.join(output_dir, f\"{safe_name}.csv\")\n",
    "\n",
    "    # Salva il DataFrame in formato CSV\n",
    "    df.to_csv(file_path, index=True)\n",
    "    print(f\"Salvato: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3efbee",
   "metadata": {},
   "source": [
    "For example, the dataset \"hard, on serve\", looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b091c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "on_serve_on_hard = pd.read_csv('feature_datasets_csv_reduced_contexts/on serve_ on hard.csv', low_memory=False)\n",
    "on_serve_on_hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe9a7c",
   "metadata": {},
   "source": [
    "<span style=\"font-size:15px\">**Preprocessing phase**</span>\n",
    "\n",
    "**Correlation analysis**: An analysis of the constructed features is now performed. All features are numerical, mostly rates and averages, so a correlation matrix through the Pearson's coefficient is computed to identify potential relationships between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed07a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "features_dataset_dir = '../feature_datasets_csv_reduced_contexts/'\n",
    "\n",
    "# Itera su tutti i file nella cartella specificata\n",
    "for dataset_name in os.listdir(features_dataset_dir):\n",
    "    # Controlla se il file Ã¨ un CSV\n",
    "    if dataset_name.endswith('.csv'):\n",
    "        # Costruisci il percorso completo del file\n",
    "        file_path = os.path.join(features_dataset_dir, dataset_name)\n",
    "        # Carica il dataset\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        feature_cols = [col for col in df.columns if col not in [\"player\"]]\n",
    "        corr_matrix = df[feature_cols].corr()\n",
    "\n",
    "    # Itera su righe e colonne della matrice di correlazione\n",
    "    for i in range(len(corr_matrix)):\n",
    "        for j in range(len(corr_matrix.columns)):\n",
    "            corr = corr_matrix.iloc[i, j]\n",
    "            if (corr > 0.6 or corr < -0.6) and i!=j:\n",
    "                print(f\"{dataset_name} riga: {corr_matrix.index[i]}, colonna: {corr_matrix.columns[j]}\")\n",
    "\n",
    "    # Visualizza la matrice di correlazione\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title(f'Matrice di Correlazione per {dataset_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e72e1ba",
   "metadata": {},
   "source": [
    "The analysis reveals that some features are highly correlated.\n",
    "First of all, the **forehand_rate** and **backhand_rate** are strongly negatively correlated in both opening shot contexts. This is quite expected, as the forehand is naturally the opposite of the backhand.\n",
    "To address this, three new features are created: `opening_1st_ground_fh_bh_ratio`, copening_2nd_ground_fh_bh_ratio`, and `response_ground_fh_bh_ratio`, which aggregate the information into a single value.<br><br>\n",
    "Secondly, the **slice_rate in the opening phase** is highly positively correlated with the overall **slice_rate**.\n",
    "This is quite expected, as they essentially represent the same information.\n",
    "Unlike the case of forehand and backhand rates, the decision here is to keep the general feature and discard the one specific to the opening phase.\n",
    "In fact, data analysis shows that the slice is used less frequently in the opening phase, resulting in very low percentages.\n",
    "Instead, the overall **slice_rate** is more informative, as it strongly characterizes the player's playing style.\n",
    "\n",
    "**Feature normalization**: all the feature are normalized using z-score normalization. This ensure all the features to have mean equal to 0 and standard deviation equal to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e8dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "features_dataset_dir = '../feature_datasets_csv_reduced_contexts/'\n",
    "features_dataset_dir_reduced_features = '../feature_datasets_csv_reduced&scaled_features/'\n",
    "\n",
    "# Iterate through all files in directory\n",
    "for dataset_name in os.listdir(features_dataset_dir):\n",
    "    if dataset_name.endswith('.csv'):\n",
    "        file_path = os.path.join(features_dataset_dir, dataset_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dest_file_path = os.path.join(features_dataset_dir_reduced_features, dataset_name)\n",
    "\n",
    "        # Calculate forehand/backhand ratios for all three shots\n",
    "        df['opening_2nd_ground_fh_bh_ratio'] = df['opening_2nd_forehand_ground'] / df['opening_2nd_backhand_ground']\n",
    "        df['opening_3rd_ground_fh_bh_ratio'] = df['opening_3rd_forehand_ground'] / df['opening_3rd_backhand_ground']\n",
    "\n",
    "        # Drop original columns\n",
    "        columns_to_drop = ['opening_2nd_forehand_ground', 'opening_2nd_backhand_ground',\n",
    "                           'opening_3rd_forehand_ground', 'opening_3rd_backhand_ground', 'opening_2nd_slice_shot','opening_3rd_slice_shot']\n",
    "\n",
    "        if 'on response' in dataset_name:\n",
    "            df['opening_1st_ground_fh_bh_ratio'] = df['opening_1st_forehand_ground'] / df['opening_1st_backhand_ground']\n",
    "            columns_to_drop.append('opening_1st_forehand_ground')\n",
    "            columns_to_drop.append('opening_1st_backhand_ground')\n",
    "            columns_to_drop.append('opening_1st_slice_shot')\n",
    "            mask1 = np.isinf(df['opening_1st_ground_fh_bh_ratio'])\n",
    "            for idx in df[mask1].index:\n",
    "                print(idx,dataset_name)\n",
    "\n",
    "        mask2 = np.isinf(df['opening_2nd_ground_fh_bh_ratio'])\n",
    "        for idx in df[mask2].index:\n",
    "            print(idx,dataset_name)\n",
    "        mask3 = np.isinf(df['opening_3rd_ground_fh_bh_ratio'])\n",
    "        for idx in df[mask3].index:\n",
    "            print(idx,dataset_name)\n",
    "            df.at[idx,'opening_3rd_ground_fh_bh_ratio'] = 24\n",
    "\n",
    "\n",
    "        df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "        # Normalize the features values with z-score normalization\n",
    "        feature_cols = [col for col in df.columns if col not in [\"player\"]]\n",
    "        X = df[feature_cols].values\n",
    "        X_scaled = StandardScaler().fit_transform(X)\n",
    "        \n",
    "        # Crea un nuovo DataFrame normalizzato \n",
    "        df_scaled = pd.DataFrame(X_scaled, columns=feature_cols)\n",
    "\n",
    "        # aggiungi di nuovo la colonna \"player\" \n",
    "        df_scaled[\"player\"] = df[\"player\"].values\n",
    "\n",
    "        # Save modified dataset\n",
    "        df_scaled.to_csv(dest_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97de80069a6830ea",
   "metadata": {},
   "source": [
    "Now the clustering analysis can start. First of all, the datasets are uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377e8d02c2d906b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dfs = []\n",
    "features_dataset_dir = 'feature_datasets_csv_reduced&scaled_features/'\n",
    "\n",
    "for dataset_name in os.listdir(features_dataset_dir):\n",
    "    if dataset_name.endswith('.csv'):\n",
    "        file_path = os.path.join(features_dataset_dir, dataset_name)\n",
    "        dfs.append((dataset_name, pd.read_csv(file_path, low_memory=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb92aea5ecf847",
   "metadata": {},
   "source": [
    "In order to understand if the datasets has a non-uniform distribution of points, so that a clustering analysis can be performed, the hopkins statistic is computed throw the function `hopkins()` in the file [run_clustering.py](run_clustering.py). A 0.75 Hopkins statistic indicates clustering tendency with a 90% confidence level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9f8b21d5d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from run_clustering import hopkins\n",
    "\n",
    "for name, dataframe in dfs:\n",
    "    feature_cols = [col for col in dataframe.columns if col not in [\"player\"]]\n",
    "    print(name)\n",
    "    mean, std_dev, values = hopkins(dataframe[feature_cols].values)\n",
    "    print(f\"Hopkins statistic (df): {mean:.3f} Â± {std_dev:.3f}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eeaf485c9a2c1878",
   "metadata": {},
   "source": [
    "The clustering approach aims to identify **four** distinct groups that effectively capture the stylistic differences in tennis, as suggested in the article from Harvard Sports Analysis [Sorting Strokes: Classifying Tennis Players Based on Stats and Style](https://harvardsportsanalysis.org/2021/07/sorting-strokes-classifying-tennis-players-based-on-stats-and-style). To achieve this, different algorithms that require the number of clusters as a parameter are tested.<br>\n",
    "\n",
    "Clustering performed on the raw data without applying any dimensionality reduction exhibits suboptimal results. Although the Hopkins statistic indicates a significant clustering tendency, the high dimensionality of the datasetsâ€”14 features for the response datasets and 17 features for the serve datasetsâ€”triggers the curse of dimensionality, adversely impacting clustering performance.\n",
    "The execution of **kmeans**, within the function `run_kmeans_clustering()` in the file [run_clustering.py](run_clustering.py), shows bad performances on silhouette score and DBI score.\n",
    "The Daviesâ€“Bouldin Index (DBI) is a metric used to evaluate the quality of clustering by assessing both intra-cluster compactness and inter-cluster separation. It does so by computing, for each cluster, a similarity measure that compares its dispersion with the distance to the nearest neighboring cluster. The final DBI score is obtained by averaging these similarity measures across all clusters. A lower DBI value indicates better clustering performance, meaning that the clusters are both compact and well-separated. Conversely, higher values suggest overlapping or loosely defined clusters. In practice, values below 1 correspond to well-defined clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7f0d29-3ec2-433a-85a5-51121a9a8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_clustering import *\n",
    "\n",
    "dfs_clustered_pca_kmeans = []\n",
    "\n",
    "for name, dataframe in dfs:\n",
    "    print(name)\n",
    "    metrics = []\n",
    "    df_clustered, silhouette_score, dbi = run_kmeans_clustering(dataframe, dimensionality_reduction = \"no reduction\")\n",
    "    dfs_clustered_pca_kmeans.append((name, df_clustered))\n",
    "    metrics.append((silhouette_score, dbi))\n",
    "    print({name: max(metrics, key=lambda x: x[0])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e780b1-63e5-4d7f-909d-f9f4c6081039",
   "metadata": {},
   "source": [
    "To improve performance, PCA is applied to reduce dimensionality while retaining 80% of the variance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba716fd-85d0-4156-a9dc-38aedfe73897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "dfs_pca=[]\n",
    "for name, dataframe in dfs:\n",
    "    feature_cols = [col for col in dataframe.columns if col not in [\"player\"]]\n",
    "    values = dataframe[feature_cols].values\n",
    "    \n",
    "    # Keep enough features to explain 80% of the variance\n",
    "    pca = PCA(n_components=0.8)  \n",
    "    X_pca = pca.fit_transform(values)\n",
    "\n",
    "    # Create column names: PCA_1, PCA_2, ..., PCA_n\n",
    "    pca_col_names = [f'PCA_{i+1}' for i in range(X_pca.shape[1])]\n",
    "    \n",
    "    # Create a new DataFrame\n",
    "    dfs_pca.append((name, pd.DataFrame(X_pca, columns=pca_col_names, index=dataframe.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99d9b2-85e3-4d44-a376-521b5bbc8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_clustering import *\n",
    "\n",
    "dfs_clustered = []\n",
    "\n",
    "for name, dataframe in dfs_pca:\n",
    "    print(name)\n",
    "    metrics = []\n",
    "    df_clustered, silhouette_score, dbi = run_kmeans_clustering(dataframe, dimensionality_reduction=\"pca\")\n",
    "    dfs_clustered.append((name, df_clustered))\n",
    "    metrics.append((silhouette_score, dbi))\n",
    "    print({name: max(metrics, key=lambda x: x[0])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffc7f27-3294-4818-929f-19a9dae16022",
   "metadata": {},
   "source": [
    "Even with PCA retaining 80% of the variance, the performance remains poor. Only when reducing the variance threshold to 40% does a slight improvement appear (still with a low silhouette coefficient around 0.25), but this comes at the cost of losing significant feature variance.<br>\n",
    "\n",
    "A solution come from another feature reduction techique, [UMAP (Uniform Manifold Approximation and Projection for Dimension Reduction)](https://umap-learn.readthedocs.io/en/latest/). From UMAP website: \"UMAP is a fairly flexible non-linear dimension reduction algorithm. It seeks to learn the manifold structure of your data and find a low dimensional embedding that preserves the essential topological structure of that manifold.\". In practice, UMAP is a nonlinear dimensionality reduction technique. It works in three main steps:\n",
    "\n",
    "* Graph Construction: UMAP builds a weighted graph where data points are connected based on their local neighborhood (using nearest neighbors). The distance between points is modeled as a probability distribution that captures local structure.\n",
    "\n",
    "* Graph Optimization: It then optimizes a low-dimensional representation (e.g., 2D or 3D) by preserving the structure of the original high-dimensional graph. This is done by minimizing the difference (cross-entropy) between the high-dimensional and low-dimensional graphs.\n",
    "\n",
    "* Embedding Output: The result is a projection of the data that preserves both local and some global relationships, making it suitable for clustering tasks.\n",
    "\n",
    "In order to evaluate how well UMAP transforms the original dataset, a metric called **trustworthiness** can be computed. Trustworthiness measures whether points that are close in the UMAP space were also close in the original space. The metric ranges from 0 to 1, where 1 indicates perfect trustworthiness, and values below 1 suggest that some distortion has been introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6c4582595d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import trustworthiness\n",
    "import umap\n",
    "\n",
    "dfs_umap_2d = []\n",
    "\n",
    "dfs_umap_8d = []\n",
    "\n",
    "umap_components = (2, 8)\n",
    "for name, dataframe in dfs:\n",
    "    feature_cols = [col for col in dataframe.columns if col not in [\"player\"]]\n",
    "    values = dataframe[feature_cols].values\n",
    "    for umap_comp in umap_components:\n",
    "        # UMAP reduction with default neighbors parameter set to 15\n",
    "        reducer = umap.UMAP(n_components=umap_comp, random_state = 42, n_jobs=1)\n",
    "        X_umap = reducer.fit_transform(values)\n",
    "        \n",
    "        # Create column names: UMAP_1, UMAP_2, ..., UMAP_n\n",
    "        umap_col_names = [f'UMAP_{i+1}' for i in range(X_umap.shape[1])]\n",
    "\n",
    "        if umap_comp == 2:\n",
    "            # Create a new DataFrame and include 'player' column\n",
    "            df_umap = pd.DataFrame(X_umap, columns=umap_col_names, index=dataframe.index)\n",
    "            df_umap['player'] = dataframe['player']\n",
    "            dfs_umap_2d.append((name, df_umap))\n",
    "        else:\n",
    "            # Create a new DataFrame and include 'player' column\n",
    "            df_umap = pd.DataFrame(X_umap, columns=umap_col_names, index=dataframe.index)\n",
    "            df_umap['player'] = dataframe['player']\n",
    "            dfs_umap_8d.append((name, df_umap))\n",
    "    \n",
    "        # trustworthiness calculation\n",
    "        trust = trustworthiness(values, X_umap, n_neighbors=15)\n",
    "        print(f\"Trustworthiness UMAP {umap_comp}D: {trust:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1529d44b-79b1-404c-b29d-042c893369a4",
   "metadata": {},
   "source": [
    "The results show a good feature transformation, with a low level of distortion in both 2 and 8 dimensions. Now, kmeans performances increase significantly, with a silhouette coefficient around 0.4 and a DBI score around 0.8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83ebe7a0ef234ed",
   "metadata": {},
   "source": [
    "KMeans clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9452a54752820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_clustering import run_kmeans_clustering\n",
    "\n",
    "dfs_clustered_kmeans2d = []\n",
    "dfs_clustered_kmeans8d = []\n",
    "\n",
    "for dfs_umap in (dfs_umap_2d, dfs_umap_8d):\n",
    "    umap_comp = 2 if dfs_umap is dfs_umap_2d else 8\n",
    "    for name, dataframe in dfs_umap:\n",
    "        print(name)\n",
    "        print(\"umap dimensions: \" + str(umap_comp))\n",
    "        metrics = []\n",
    "        df_clustered = run_kmeans_clustering(dataframe, dimensionality_reduction=\"umap\")\n",
    "        if umap_comp == 2:\n",
    "            dfs_clustered_kmeans2d.append((name, df_clustered))\n",
    "        else:\n",
    "            dfs_clustered_kmeans8d.append((name, df_clustered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cbc4b056ea3c4b",
   "metadata": {},
   "source": [
    "Another type of clustering algorithm is now tested, using the UMAP reduction. A hierarchical solution can be useful, since we can specify the number of cluster needed, so that the resulting dendogram can be cutted to obain that number. In particular the `AgglomerativeClustering()` function from **sklearn** id used, inside `run_umap_agglomerative_clustering` in [run_clustering.py](run_clustering.py) file..\n",
    "Agglomerative clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de596699b8daf97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_clustering import run_umap_agglomerative_clustering\n",
    "\n",
    "dfs_clustered_agglomerative_complete2d = []\n",
    "dfs_clustered_agglomerative_average2d = []\n",
    "dfs_clustered_agglomerative_single2d = []\n",
    "\n",
    "dfs_clustered_agglomerative_complete8d = []\n",
    "dfs_clustered_agglomerative_average8d = []\n",
    "dfs_clustered_agglomerative_single8d = []\n",
    "\n",
    "for dfs_umap in (dfs_umap_2d, dfs_umap_8d):\n",
    "    umap_comp = 2 if dfs_umap is dfs_umap_2d else 8\n",
    "    for name, dataframe in dfs_umap:\n",
    "        print(name)\n",
    "        print(\"umap dimensions: \" + str(umap_comp))\n",
    "        metrics = []\n",
    "\n",
    "        # run with complete, single, average linkage methods\n",
    "        print(\"linkage method: complete\")\n",
    "        df_clustered = run_umap_agglomerative_clustering(dataframe, linkage='complete')\n",
    "        \n",
    "        print(\"linkage method: average\")\n",
    "        df_clustered = run_umap_agglomerative_clustering(dataframe, linkage='average')\n",
    "        \n",
    "        print(\"linkage method: single\")\n",
    "        df_clustered = run_umap_agglomerative_clustering(dataframe, linkage='single')\n",
    "        if umap_comp == 2:\n",
    "            # adding clustered datasets to the lists\n",
    "            dfs_clustered_agglomerative_complete2d.append((name, df_clustered))\n",
    "            dfs_clustered_agglomerative_average2d.append((name, df_clustered))\n",
    "            dfs_clustered_agglomerative_single2d.append((name, df_clustered))\n",
    "        else:\n",
    "            # adding clustered datasets to the lists\n",
    "            dfs_clustered_agglomerative_complete8d.append((name, df_clustered))\n",
    "            dfs_clustered_agglomerative_average8d.append((name, df_clustered))\n",
    "            dfs_clustered_agglomerative_single8d.append((name, df_clustered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ba790-b0b7-40a1-b1a8-23d67cdf1ef5",
   "metadata": {},
   "source": [
    "Running the algorithms the results are clear, the agglomerative clustering performs well with **complete** and **average** linkage methods, while performs poorly with **single**. This behaviour can be explained by the fact that single linkage tends to form long chain-like clusters, often ignoring natural, well-separated groupings. In contrast, the other linkage methods are better at recognizing distinct cluster shapes. However, agglomerative clustering does not show a significant improvement over KMeans. On the contrary, the metrics indicate a slight drop in performance, with a clear advantage for 2D UMAP over 8D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83601a6eaa50b37",
   "metadata": {},
   "source": [
    "Even if the number of cluster parameter cannot being specified, a density based algorithm, OPTICS, is tested, hoping to find a good clustering with the desired number of clusters. The algorithm specifies these parameters: \n",
    "* **min_pts**: The minimum number of points to form a dense region in OPTICS clustering. Defaults to 3.\n",
    "* **xi**: A parameter for OPTICS that determines the minimum steepness on the reachability plot to identify clusters. Defaults to 0.08.\n",
    "* **min_cluster_size**: The minimum relative size of a cluster in OPTICS as a fraction of the total number of data points. Defaults to 0.1.\n",
    "\n",
    "OPTICS was tested using this choice of parameters. A noise percentage threshold was fixed at 30%, meaning that the silhouette score was computed only when the number of points classified as noise did not exceed 30% of the total. By varying the algorithm parameters, it becomes clear that, regardless of the configuration, the percentage of noise remains high. This indicates that the point structure lacks clearly defined dense regions. Even in datasets where such regions exist, the large number of noise points makes the use of OPTICS ineffective, given the goal of assigning each player to a cluster.\n",
    "\n",
    "OPTICS clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247fd93e184344a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_clustering import run_umap_optics_clustering\n",
    "\n",
    "dfs_clustered_optics = []\n",
    "\n",
    "for dfs_umap in (dfs_umap_2d, dfs_umap_8d):\n",
    "    umap_comp = 2 if dfs-umap is dfs_umap_2d else 8\n",
    "    for name, dataframe in dfs_umap:\n",
    "        print(name)\n",
    "        print(\"umap dimensions: \" + str(umap_comp))\n",
    "        metrics = []\n",
    "        df_clustered = run_umap_optics_clustering(dataframe)\n",
    "        dfs_clustered_optics.append((name, df_clustered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393c88d3dcc9471a",
   "metadata": {},
   "source": [
    "A GMM (Gaussian Mixture Model) is a probabilistic clustering algorithm that assumes the data is generated from a mixture of several multivariate Gaussian distributions.\n",
    "GMM does not assign points rigidly, instead, it computes for each point the probability of belonging to each cluster. \n",
    "Each cluster is represented by a multivariate Gaussian distribution, defined by:\n",
    "* A mean vector (cluster center),\n",
    "* A covariance matrix (cluster shape and orientation),\n",
    "* A prior probability (how large or likely the cluster is overall).\n",
    "\n",
    "The model learns all these parameters from the data.\n",
    "\n",
    "**Algorithm steps**:\n",
    "\n",
    "1. Initialization: The model starts with initial guesses for the parameters (e.g., using KMeans or random initialization).\n",
    "\n",
    "2. Expectation Step (E-step):\n",
    "For each data point, compute the posterior probability of belonging to each Gaussian component (i.e., each cluster).\n",
    "\n",
    "3. Maximization Step (M-step):\n",
    "Update the parameters (means, covariances, and priors) to maximize the likelihood of the data given these soft assignments.\n",
    "\n",
    "4. Repeat until convergence:\n",
    "The algorithm iteratively alternates between E-step and M-step until the model stabilizes (i.e., changes in probabilities or parameters are below a threshold).\n",
    "\n",
    "This soft clustering approach is useful for analyzing whether some players have hybrid statistics, meaning they do not clearly belong to any single cluster with high certainty.\n",
    "\n",
    "The algorithm will find elliptical shaped clusters and takes as parameter the number of clusters deisred. The function `run_umap_gmm_clustering()` runs the algorithm and uses, by default, a probability threshold of 90%. If a point does not belong to any cluster with a probability above this threshold, it is considered ambiguous.\n",
    "\n",
    "The algorithm version used is [GaussianMixture() provided by scikit-learn.](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)\n",
    "\n",
    "GMM clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d1abc206a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_clustering import run_umap_gmm_clustering\n",
    "\n",
    "dfs_clustered_gmm = []\n",
    "\n",
    "for dfs_umap in (dfs_umap_2d, dfs_umap_8d):\n",
    "    umap_comp = 2 if dfs_umap is dfs_umap_2d else 8\n",
    "    for name, dataframe in dfs_umap:\n",
    "        print(name)\n",
    "        print(\"umap dimensions: \" + str(umap_comp))\n",
    "        metrics = []\n",
    "        df_clustered = run_umap_gmm_clustering(dataframe)\n",
    "        dfs_clustered_optics.append((name, df_clustered))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361bbd5-5612-42d9-b424-1a1dde74f93e",
   "metadata": {},
   "source": [
    "The results show that when using UMAP with 2 components, there are slightly more ambiguous points, whereas with 8 components the number of ambiguous points decreases. This is likely because, although UMAP in 2D preserves local structure well (high trustworthiness), the representation is too compressed for the GMM to effectively separate clusters, resulting in lower membership probabilities. On the other hand, the scores obtained in 8D â€” calculated excluding ambiguous points â€” are not very high and do not demonstrate an improvement compared to KMeans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4906fbf74618f7",
   "metadata": {},
   "source": [
    "Agglomerate statistic by cluster visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ea9c52117db76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize_results import cluster_feature_deltas\n",
    "from visualize_results import aggregate_features_by_cluster\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# stampa le features aggregate per cluster\n",
    "aggregated = aggregate_features_by_cluster(df_clustered_kmeans)\n",
    "print(aggregated)\n",
    "\n",
    "# stampa la hitmap della differenza nelle statistiche tra i clusters e la media globale\n",
    "sns.heatmap(cluster_feature_deltas(df_clustered_kmeans), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Scostamenti medi delle feature per cluster\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f19810eea7d6e",
   "metadata": {},
   "source": [
    "<span style=\"font-size:15px\">**Results visualization**</span>\n",
    "\n",
    "Once the clusters are formed, the playing styles can be visualized. For each cluster, the centroid is computed, and the deviation of each feature from the overall dataset mean is calculated. This provides general insight into the characteristics of each cluster, allowing for a textual description to be written. \n",
    "Since KMeans UMAP 2D appears to be the most effective algorithm, the visualization is based on its clusters, and the clustered datasets are saved into files. These datasets contain UMAP components, so in order to visualize the results, the cluster labels must be inserted into the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0e26b43fa200fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "features_dataset_clustered_dir = 'feature_datasets_csv_reduced_contexts&features_clustered_kmeans'\n",
    "original_feature_dataset = 'feature_datasets_csv_reduced_contexts&features'\n",
    "\n",
    "dfs_not_scaled=[]\n",
    "for dataset_name in os.listdir(original_feature_dataset):\n",
    "    if dataset_name.endswith('.csv'):\n",
    "        file_path = os.path.join(original_feature_dataset, dataset_name)\n",
    "        dfs_not_scaled.append((dataset_name, pd.read_csv(file_path, low_memory=False)))\n",
    "        \n",
    "for name_clustered, df_clustered in dfs_clustered_kmeans2d:\n",
    "    for name_original, df in dfs_not_scaled:\n",
    "        if name_original == name_clustered:\n",
    "            # the column \"cluster\" is added to the original dataset\n",
    "            df[\"cluster\"] = df_clustered[\"cluster\"]\n",
    "            file_path = os.path.join(features_dataset_clustered_dir, name_original)\n",
    "            df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1da5b35010d56",
   "metadata": {},
   "source": [
    "Upload dfs clustered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617d46a04b7bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "features_dataset_clustered_dir = 'feature_datasets_csv_reduced_contexts&features_clustered_kmeans'\n",
    "dfs_clustered = []\n",
    "for dataset_name in os.listdir(features_dataset_clustered_dir):\n",
    "    if dataset_name.endswith('.csv'):\n",
    "        file_path = os.path.join(features_dataset_clustered_dir, dataset_name)\n",
    "        dfs_clustered.append((dataset_name, pd.read_csv(file_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85cad5-48fa-4c5c-9e20-7317a786abe9",
   "metadata": {},
   "source": [
    "Now, for each dataset, a list of the 5 most representative players for each cluster is extracted, in order to provide player examples for each cluster visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1690ee58-e563-452f-b7c0-2df0eec09ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "features_dataset_dir = 'feature_datasets_csv_reduced_contexts&features_clustered_kmeans'\n",
    "for dataset_name in os.listdir(features_dataset_dir):\n",
    "    players_per_cluster = {}\n",
    "    file_path = os.path.join(features_dataset_dir, dataset_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_grouped=df.groupby('cluster')\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    for cluster_id, group in df_grouped:\n",
    "        players = group['player']\n",
    "        print(f\"Cluster {cluster_id}: {list(players)}\\n\\n\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# dizionario che salva le liste e il contesto\n",
    "clusters = {\n",
    "    \"on response_ on clay, cluster 0\": [\"Alexander_Zverev\", \"Casper_Ruud\", \"Jack_Draper\", \"Jannik_Sinner\", \"Taylor_Fritz\"],\n",
    "    \"on response_ on clay, cluster 1\": [\"Andrey_Rublev\", \"Arthur_Fils\", \"Ben_Shelton\", \"Novak_Djokovic\", \"Grigor_Dimitrov\"],\n",
    "    \"on response_ on clay, cluster 2\": [\"Carlos_Alcaraz\", \"Felix_Auger_Aliassime\", \"Karen_Khachanov\", \"Sebastian_Korda\", \"Stefanos_Tsitsipas\"],\n",
    "    \"on response_ on clay, cluster 3\": [\"Alex_De_Minaur\", \"Daniil_Medvedev\", \"Holger_Rune\", \"Lorenzo_Musetti\", \"Gael_Monfils\"],\n",
    "    \"on response_ on grass, cluster 0\": [\"Alexander_Bublik\", \"Arthur_Fils\", \"Daniil_Medvedev\", \"Stefanos_Tsitsipas\", \"Taylor_Fritz\"],\n",
    "    \"on response_ on grass, cluster 1\": [\"Grigor_Dimitrov\", \"Hubert_Hurkacz\", \"John_Isner\", \"Sebastian_Korda\", \"Jiri_Lehecka\"],\n",
    "    \"on response_ on grass, cluster 2\": [\"Andrey_Rublev\", \"Ben_Shelton\", \"Cameron_Norrie\", \"Jannik_Sinner\", \"Matteo_Berrettini\"],\n",
    "    \"on response_ on grass, cluster 3\": [\"Alex_De_Minaur\", \"Alexander_Zverev\", \"Carlos_Alcaraz\", \"Novak_Djokovic\", \"Jack_Draper\"],\n",
    "    \"on response_ on hard, cluster 0\": [\"Alex_De_Minaur\", \"Andy_Murray\", \"Daniil_Medvedev\", \"Gael_Monfils\", \"Botic_Van_De_Zandschulp\"],\n",
    "    \"on response_ on hard, cluster 1\": [\"Arthur_Fils\", \"Ben_Shelton\", \"Casper_Ruud\", \"Holger_Rune\", \"Matteo_Berrettini\"],\n",
    "    \"on response_ on hard, cluster 2\": [\"Alexander_Zverev\", \"Carlos_Alcaraz\", \"Jack_Draper\", \"Jannik_Sinner\", \"Stefanos_Tsitsipas\"],\n",
    "    \"on response_ on hard, cluster 3\": [\"Grigor_Dimitrov\", \"Hubert_Hurkacz\", \"Lorenzo_Musetti\", \"Daniel_Evans\", \"Tallon_Griekspoor\"],\n",
    "    \"on serve_ on clay, cluster 0\": [\"Arthur_Fils\", \"Cameron_Norrie\", \"Jiri_Lehecka\", \"Sebastian_Korda\", \"Tommy_Paul\"],\n",
    "    \"on serve_ on clay, cluster 1\": [\"Alex_De_Minaur\", \"Jannik_Sinner\", \"Botic_Van_De_Zandschulp\", \"Jesper_De_Jong\", \"Thanasi_Kokkinakis\"],\n",
    "    \"on serve_ on clay, cluster 2\": [\"Carlos_Alcaraz\", \"Daniil_Medvedev\", \"Grigor_Dimitrov\", \"Holger_Rune\", \"Novak_Djokovic\"],\n",
    "    \"on serve_ on clay, cluster 3\": [\"Alexander_Zverev\", \"Ben_Shelton\", \"Casper_Ruud\", \"Matteo_Berrettini\", \"Taylor_Fritz\"],\n",
    "    \"on serve_ on grass, cluster 0\": [\"Arthur_Fils\", \"Gael_Monfils\", \"Lorenzo_Musetti\", \"Tommy_Paul\", \"Ugo_Humbert\"],\n",
    "    \"on serve_ on grass, cluster 1\": [\"Alexander_Bublik\", \"Casper_Ruud\", \"Grigor_Dimitrov\", \"Hubert_Hurkacz\", \"Matteo_Berrettini\"],\n",
    "    \"on serve_ on grass, cluster 2\": [\"Alex_De_Minaur\", \"Carlos_Alcaraz\", \"Frances_Tiafoe\", \"Jannik_Sinner\", \"Marton_Fucsovics\"],\n",
    "    \"on serve_ on grass, cluster 3\": [\"Alexander_Zverev\", \"Ben_Shelton\", \"Jack_Draper\", \"Novak_Djokovic\", \"Taylor_Fritz\"],\n",
    "    \"on serve_ on hard, cluster 0\": [\"Alexander_Zverev\", \"Andrey_Rublev\", \"Casper_Ruud\", \"Holger_Rune\", \"Taylor_Fritz\"],\n",
    "    \"on serve_ on hard, cluster 1\": [\"Arthur_Fils\", \"Ben_Shelton\", \"Hubert_Hurkacz\", \"Jack_Draper\", \"Novak_Djokovic\"],\n",
    "    \"on serve_ on hard, cluster 2\": [\"Andy_Murray\", \"Carlos_Alcaraz\", \"Grigor_Dimitrov\", \"Lorenzo_Musetti\", \"Matteo_Berrettini\"],\n",
    "    \"on serve_ on hard, cluster 3\": [\"Adrian_Mannarino\", \"Alex_De_Minaur\", \"Aslan_Karatsev\", \"Jannik_Sinner\", \"Juan_Manuel_Cerundolo\"]\n",
    "}\n",
    "\n",
    "top_players_per_context_dir = '../top_players_per_context/'\n",
    "\n",
    "for context, list_of_players in clusters.items():\n",
    "    dest_file_path = os.path.join(top_players_per_context_dir, context)\n",
    "    df = pd.DataFrame({'player': list_of_players})\n",
    "    df.to_csv(dest_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77d284f3a571c30",
   "metadata": {},
   "source": [
    "Calculate for each context, for each cluster, the top n representative features with how much in percentage the centroid of the cluster differs from the overall mean, and then printed. The list of the most important player belonging to the cluster is also shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb75843b14ea735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize_results import analyze_cluster_profiles\n",
    "\n",
    "for name, df_clustered in dfs_clustered:\n",
    "    print(\"\\n\"+name)\n",
    "    analyze_cluster_profiles(df_clustered, context=name.replace(\".csv\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eed2ad5b42959de",
   "metadata": {},
   "source": [
    "It's also interesting to understand how clusters across different contexts are related. For example, it is worth checking whether a player who belongs to cluster 1 on grass (on return) also falls into a cluster with similar characteristics on clay (on return).\n",
    "\n",
    "To explore this, two similarity matrix is computed, one for serve context and the other for response, measuring the distance between each cluster in one context and all clusters in the other contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7adf8e9f28f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize_results import calculate_centroid_similarity\n",
    "\n",
    "# 3. Computing the centroid similarity\n",
    "clustered_data_on_serve = []\n",
    "clustered_data_on_response = []\n",
    "for context, df_clustered in dfs_clustered:\n",
    "    if \"on serve\" in context:\n",
    "        clustered_data_on_serve.append((context, df_clustered))\n",
    "    else:\n",
    "        clustered_data_on_response.append((context, df_clustered))\n",
    "similarity_df_on_serve = calculate_centroid_similarity(clustered_data_on_serve)\n",
    "similarity_df_on_response = calculate_centroid_similarity(clustered_data_on_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f26332ca521140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize_results import visualize_similarity_matrix\n",
    "\n",
    "# 4. Similarity matrix visualized\n",
    "visualize_similarity_matrix(similarity_df_on_serve, clustered_data_on_serve)\n",
    "visualize_similarity_matrix(similarity_df_on_response, clustered_data_on_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759717984790027",
   "metadata": {},
   "source": [
    "Finally, a player â€” such as Roger Federer in this example â€” can view his trajectory across various contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad47f1675b03c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize_results import create_player_trajectories, visualize_player_trajectory\n",
    "\n",
    "visualize_player_trajectory(create_player_trajectories(dfs_clustered), \"Roger_Federer\", dfs_clustered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa4cfcd62307fb7",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px\">**Sequential pattern mining Analysis**</span>\n",
    "\n",
    "In order to provide players with a tool to better understand their most frequently used shot sequences, an analysis of the point opening phase is performed. In particular, the function `frequent_shots_py_player()` in the file [get_freq_shots_seqs.py](get_freq_shots_seqs.py) returns the frequent sequential patterns of the first three shots in the points, based on a predefined minimum support threshold set within the function.\n",
    "\n",
    "It is important to note that this approach considers only the **first three consecutive** shots and does not perform a full sequential pattern analysis, which would include also **non-consecutive** shots. While the original idea was to implement this broader type of analysis, it was ultimately deemed less useful: sequences of non-consecutive shots within a point lack clear structure and are harder to interpret meaningfully. On the other hand, analyzing three consecutive shots provides more interpretable and actionable insights for players.\n",
    "\n",
    "However, future work in this direction could be valuable, aiming to better understand potential relationships among non-consecutive shots within a point.\n",
    "\n",
    "Although full sequential pattern analysis algorithms are not necessary for this specific purpose, `seqmining()`, an FP-growth like algorithm, was still tested and compared with a simple frequency count of the most common opening-phase triplets. The results in terms of computational time are clear: the `seqmining()` algorithm is not suitable for this task, as its execution time is significantly higher than that of a basic counting approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf7c5c6-0320-4534-9e61-c6c55f8eea9f",
   "metadata": {},
   "source": [
    "`Counter()` approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1c4250a1f5e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from costants import PLAYER_SURFACES_DICT\n",
    "from decode_point import decode_point\n",
    "from get_freq_shots_seqs import frequent_shots_py_player\n",
    "\n",
    "# calculation using Counter()\n",
    "\n",
    "# original dataset\n",
    "df_points = pd.read_csv('points_datasets/charting-m-points-2020s.csv', low_memory=False)\n",
    "\n",
    "print(\"Legend: * = winner, # = forced error, @ = unforced error\\n\")\n",
    "\n",
    "# iterate on all the context surface-type of point for the player Carlos Alcaraz\n",
    "for surface in PLAYER_SURFACES_DICT.get(\"Carlos_Alcaraz\", []):\n",
    "    for context in [\"on serve\", \"on response\"]:\n",
    "        print(\"\\n\" + context + \" on \" + surface + \"\\n\")\n",
    "        \n",
    "        # the frequent shot sequences are returned\n",
    "        for (sequence, support, win_percentage, most_frequent_outcome), time_duration in frequent_shots_py_player(\"Carlos_Alcaraz\",df_points, surface, context):\n",
    "            shots = []\n",
    "            \n",
    "            # the sequence is decoded in a human readable format \n",
    "            for shot in sequence:\n",
    "                shots.append(decode_point(shot))\n",
    "                \n",
    "            # the sequences are returned sorted by support, from the higher to the lowest\n",
    "            print(\n",
    "                f\"Sequence: {shots} | Support: {support} | Win percentage: {win_percentage} | Most frequent outcome: {most_frequent_outcome}\\n\")\n",
    "        print(\"Time duration:\", time_duration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbb9655-4d66-49d2-bb35-89872b0d4231",
   "metadata": {},
   "source": [
    "`Seqmining()` approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c1b018-d72e-491d-b94c-a3c03332f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from costants import PLAYER_SURFACES_DICT\n",
    "from decode_point import decode_point\n",
    "from get_freq_shots_seqs import frequent_shots_py_player\n",
    "\n",
    "# calculation using seqmining()\n",
    "\n",
    "# original dataset\n",
    "df_points = pd.read_csv('points_datasets/charting-m-points-2020s.csv', low_memory=False)\n",
    "\n",
    "print(\"Legend: * = winner, # = forced error, @ = unforced error\\n\")\n",
    "\n",
    "# iterate on all the context surface-type of point for the player Carlos Alcaraz\n",
    "for surface in PLAYER_SURFACES_DICT.get(\"Carlos_Alcaraz\", []):\n",
    "    for context in [\"on serve\", \"on response\"]:\n",
    "        print(\"\\n\" + context + \" on \" + surface + \"\\n\")\n",
    "        \n",
    "        # the frequent shot sequences are returned\n",
    "        for (sequence, support, win_percentage, most_frequent_outcome), time_duration in frequent_shots_py_player(\"Carlos_Alcaraz\",df_points, surface, context, seqmining=True):\n",
    "            shots = []\n",
    "            \n",
    "            # the sequence is decoded in a human readable format \n",
    "            for shot in sequence:\n",
    "                shots.append(decode_point(shot))\n",
    "                \n",
    "            # the sequences are returned sorted by support, from the higher to the lowest\n",
    "            print(\n",
    "                f\"Sequence: {shots} | Support: {support} | Win percentage: {win_percentage} | Most frequent outcome: {most_frequent_outcome}\\n\")\n",
    "        print(\"Time duration:\", time_duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c12a30a-3c4d-45ab-9e22-c580691d1010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
